{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Movie Success Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our lightweight pipelines components using Python\n",
    "\n",
    "\n",
    "#### Lightweight python components\n",
    "\n",
    "\n",
    "Lightweight python components do not require you to build a new container image for every code change. They're intended to use for fast iteration in notebook environment.\n",
    "\n",
    "#### Building a lightweight python component\n",
    "\n",
    "To build a component just define a stand-alone python function and then call kfp.components.func_to_container_op(func) to convert it to a component that can be used in a pipeline.\n",
    "\n",
    "There are several requirements for the function:\n",
    "\n",
    "- The function should be stand-alone. It should not use any code declared outside of the function definition. Any imports should be added inside the main function. Any helper functions should also be defined inside the main function.\n",
    "\n",
    "- The function can only import packages that are available in the base image. If you need to import a package that's not available you can try to find a container image that already includes the required packages. (As a workaround you can use the module subprocess to run pip install for the required package.)\n",
    "\n",
    "- If the function operates on numbers, the parameters need to have type hints. Supported types are [int, float, bool]. Everything else is passed as string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Python function-based components\n",
    "\n",
    "A Kubeflow Pipelines component is a self-contained set of code that performs one step in your ML workflow. A pipeline component is composed of:\n",
    "\n",
    "- The component code, which implements the logic needed to perform a step in your ML workflow.\n",
    "\n",
    "- A component specification, which defines the following:\n",
    "\n",
    "    - The component's metadata, its name and description.\n",
    "    - The component's interface, the component's inputs and outputs.\n",
    "    - The component's implementation, the Docker container image to run, how to pass inputs to your component code, and how to get the component's outputs.\n",
    "    \n",
    "\n",
    "Python function-based components make it easier to iterate quickly by letting you build your component code as a Python function and generating the component specification for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.local/lib/python3.6/site-packages (20.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-20.3.2-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'pip' candidate (version 20.3.2 at https://files.pythonhosted.org/packages/3d/0c/01014c0442830eb38d6baef0932fdcb389279ce74295350ecb9fe09e048a/pip-20.3.2-py2.py3-none-any.whl#sha256=8d779b6a85770bc5f624b5c8d4d922ea2e3cd9ce6ee92aa260f12a9f072477bc (from https://pypi.org/simple/pip/) (requires-python:>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*))\n",
      "Reason for being yanked: <none given>\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.3.1\n",
      "    Uninstalling pip-20.3.1:\n",
      "      Successfully uninstalled pip-20.3.1\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.6 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-20.3.2\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.11.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.2)\n",
      "Requirement already satisfied: keras in ./.local/lib/python3.6/site-packages (2.4.3)\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.6/site-packages (0.11.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (5.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./.local/lib/python3.6/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.26.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (0.16.1)\n",
      "Requirement already satisfied: IPython in /usr/local/lib/python3.6/dist-packages (7.11.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.1)\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (366 kB)\n",
      "\u001b[K     |████████████████████████████████| 366 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonlib-python3\n",
      "  Downloading jsonlib-python3-1.6.1.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s  eta 0:00:01\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib==1.25.9\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for urllib==1.25.9\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy in ./.local/lib/python3.6/site-packages (2.3.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./.local/lib/python3.6/site-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.local/lib/python3.6/site-packages (from spacy) (4.54.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./.local/lib/python3.6/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./.local/lib/python3.6/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.local/lib/python3.6/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.local/lib/python3.6/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./.local/lib/python3.6/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./.local/lib/python3.6/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./.local/lib/python3.6/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.local/lib/python3.6/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (45.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (2.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython \n",
    "!python -m pip install pandas \n",
    "!python -m pip install pandas matplotlib scipy scikit-learn tensorflow keras seaborn --user\n",
    "!python -m pip install IPython numpy imblearn tensorboard wordcloud jsonlib-python3 urllib==1.25.9 --user\n",
    "!python -m pip install spacy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and packages for getting/reading data from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fs-gcsfs in ./.local/lib/python3.6/site-packages (1.4.1)\n",
      "Requirement already satisfied: fs~=2.0 in ./.local/lib/python3.6/site-packages (from fs-gcsfs) (2.4.11)\n",
      "Requirement already satisfied: google-cloud-storage~=1.0 in /usr/local/lib/python3.6/dist-packages (from fs-gcsfs) (1.25.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from fs~=2.0->fs-gcsfs) (45.1.0)\n",
      "Requirement already satisfied: six~=1.10 in /usr/lib/python3/dist-packages (from fs~=2.0->fs-gcsfs) (1.11.0)\n",
      "Requirement already satisfied: appdirs~=1.4.3 in ./.local/lib/python3.6/site-packages (from fs~=2.0->fs-gcsfs) (1.4.4)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fs~=2.0->fs-gcsfs) (2019.3)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage~=1.0->fs-gcsfs) (1.11.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage~=1.0->fs-gcsfs) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage~=1.0->fs-gcsfs) (0.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-storage~=1.0->fs-gcsfs) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-storage~=1.0->fs-gcsfs) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-storage~=1.0->fs-gcsfs) (0.2.8)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (1.51.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (3.11.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (2.22.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.9.0->google-cloud-storage~=1.0->fs-gcsfs) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage~=1.0->fs-gcsfs) (1.25.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gcsfs in ./.local/lib/python3.6/site-packages (0.7.1)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.6/site-packages (from gcsfs) (3.7.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.22.0)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in ./.local/lib/python3.6/site-packages (from gcsfs) (0.8.4)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.11.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in ./.local/lib/python3.6/site-packages (from gcsfs) (0.4.2)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.2->gcsfs) (1.11.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (45.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.6/site-packages (from aiohttp->gcsfs) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (19.3.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in ./.local/lib/python3.6/site-packages (from aiohttp->gcsfs) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.6/site-packages (from aiohttp->gcsfs) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in ./.local/lib/python3.6/site-packages (from aiohttp->gcsfs) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in ./.local/lib/python3.6/site-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from idna-ssl>=1.0->aiohttp->gcsfs) (2.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.11.28)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.6/site-packages (0.8.4)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install fs-gcsfs\n",
    "!python -m pip install gcsfs\n",
    "!python -m pip install fsspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install or update the pipelines SDK\n",
    "\n",
    "\n",
    "### Run the following command to install the Kubeflow Pipelines SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.1.2)\n",
      "Collecting kfp\n",
      "  Using cached kfp-1.1.2-py3-none-any.whl\n",
      "  Downloading kfp-1.1.1.tar.gz (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-1.1.0.tar.gz (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 21.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-1.0.4.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-1.0.3.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-1.0.1.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-1.0.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.5.1.tar.gz (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.5.0.tar.gz (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 17.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.4.0.tar.gz (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.3.0.tar.gz (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.4.1.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.4.tar.gz (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.2.1.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.2.tar.gz (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.1.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 19.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.2.0.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.40.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.39.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.38.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.37.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.36.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.35.tar.gz (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.34.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.33.tar.gz (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.32.2.tar.gz (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.32.1.tar.gz (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 10.6 MB/s ta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.32.tar.gz (102 kB)\n",
      "\u001b[K     |████████████████████████████████| 102 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.31.2.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.31.1.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.31.tar.gz (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.30.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.29.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.27.tar.gz (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.26.tar.gz (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.25.tar.gz (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.7 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.24.tar.gz (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.23.1.tar.gz (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 has different version in metadata: '0.1.23'\u001b[0m\n",
      "\u001b[?25h  Downloading kfp-0.1.23.tar.gz (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 has different version in metadata: '0.1.22'\u001b[0m\n",
      "\u001b[?25h  Downloading kfp-0.1.19.tar.gz (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.16.tar.gz (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.11-py3-none-any.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading kfp-0.1.11.tar.gz (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.11.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.3.1)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.1b1 in ./.local/lib/python3.6/site-packages (from kfp) (1.1.2rc1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (45.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.6.1->kfp) (1.11.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.8)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.1.2)\n",
      "Collecting kfp\n",
      "  Using cached kfp-1.1.2-py3-none-any.whl\n",
      "  Using cached kfp-1.1.1.tar.gz (162 kB)\n",
      "  Using cached kfp-1.1.0.tar.gz (172 kB)\n",
      "  Using cached kfp-1.0.4.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.3.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.1.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.0.tar.gz (116 kB)\n",
      "  Using cached kfp-0.5.1.tar.gz (119 kB)\n",
      "  Using cached kfp-0.5.0.tar.gz (118 kB)\n",
      "  Using cached kfp-0.4.0.tar.gz (118 kB)\n",
      "  Using cached kfp-0.3.0.tar.gz (117 kB)\n",
      "  Using cached kfp-0.2.5.tar.gz (116 kB)\n",
      "  Using cached kfp-0.2.4.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.4.tar.gz (115 kB)\n",
      "  Using cached kfp-0.2.2.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.2.tar.gz (114 kB)\n",
      "  Using cached kfp-0.2.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.0.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.40.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.39.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.38.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.37.tar.gz (111 kB)\n",
      "  Using cached kfp-0.1.36.tar.gz (111 kB)\n",
      "  Using cached kfp-0.1.35.tar.gz (105 kB)\n",
      "  Using cached kfp-0.1.34.tar.gz (104 kB)\n",
      "  Using cached kfp-0.1.33.tar.gz (103 kB)\n",
      "  Using cached kfp-0.1.32.2.tar.gz (101 kB)\n",
      "  Using cached kfp-0.1.32.1.tar.gz (100 kB)\n",
      "  Using cached kfp-0.1.32.tar.gz (102 kB)\n",
      "  Using cached kfp-0.1.31.2.tar.gz (99 kB)\n",
      "  Using cached kfp-0.1.31.1.tar.gz (97 kB)\n",
      "  Using cached kfp-0.1.31.tar.gz (94 kB)\n",
      "  Using cached kfp-0.1.30.tar.gz (88 kB)\n",
      "  Using cached kfp-0.1.29.tar.gz (88 kB)\n",
      "  Using cached kfp-0.1.27.tar.gz (80 kB)\n",
      "  Using cached kfp-0.1.26.tar.gz (81 kB)\n",
      "  Using cached kfp-0.1.25.tar.gz (76 kB)\n",
      "  Using cached kfp-0.1.24.tar.gz (74 kB)\n",
      "  Using cached kfp-0.1.23.1.tar.gz (73 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 has different version in metadata: '0.1.23'\u001b[0m\n",
      "  Using cached kfp-0.1.23.tar.gz (73 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 has different version in metadata: '0.1.22'\u001b[0m\n",
      "  Using cached kfp-0.1.19.tar.gz (67 kB)\n",
      "  Using cached kfp-0.1.16.tar.gz (148 kB)\n",
      "  Using cached kfp-0.1.11-py3-none-any.whl (212 kB)\n",
      "  Using cached kfp-0.1.11.tar.gz (130 kB)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.1b1 in ./.local/lib/python3.6/site-packages (from kfp) (1.1.2rc1)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.3.1)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.11.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (45.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.6.1->kfp) (1.11.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.1.2)\n",
      "Collecting kfp\n",
      "  Using cached kfp-1.1.2-py3-none-any.whl\n",
      "  Using cached kfp-1.1.1.tar.gz (162 kB)\n",
      "  Using cached kfp-1.1.0.tar.gz (172 kB)\n",
      "  Using cached kfp-1.0.4.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.3.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.1.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.0.tar.gz (116 kB)\n",
      "  Using cached kfp-0.5.1.tar.gz (119 kB)\n",
      "  Using cached kfp-0.5.0.tar.gz (118 kB)\n",
      "  Using cached kfp-0.4.0.tar.gz (118 kB)\n",
      "  Using cached kfp-0.3.0.tar.gz (117 kB)\n",
      "  Using cached kfp-0.2.5.tar.gz (116 kB)\n",
      "  Using cached kfp-0.2.4.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.4.tar.gz (115 kB)\n",
      "  Using cached kfp-0.2.2.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.2.tar.gz (114 kB)\n",
      "  Using cached kfp-0.2.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.0.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.40.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.39.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.38.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.37.tar.gz (111 kB)\n",
      "  Using cached kfp-0.1.36.tar.gz (111 kB)\n",
      "  Using cached kfp-0.1.35.tar.gz (105 kB)\n",
      "  Using cached kfp-0.1.34.tar.gz (104 kB)\n",
      "  Using cached kfp-0.1.33.tar.gz (103 kB)\n",
      "  Using cached kfp-0.1.32.2.tar.gz (101 kB)\n",
      "  Using cached kfp-0.1.32.1.tar.gz (100 kB)\n",
      "  Using cached kfp-0.1.32.tar.gz (102 kB)\n",
      "  Using cached kfp-0.1.31.2.tar.gz (99 kB)\n",
      "  Using cached kfp-0.1.31.1.tar.gz (97 kB)\n",
      "  Using cached kfp-0.1.31.tar.gz (94 kB)\n",
      "  Using cached kfp-0.1.30.tar.gz (88 kB)\n",
      "  Using cached kfp-0.1.29.tar.gz (88 kB)\n",
      "  Using cached kfp-0.1.27.tar.gz (80 kB)\n",
      "  Using cached kfp-0.1.26.tar.gz (81 kB)\n",
      "  Using cached kfp-0.1.25.tar.gz (76 kB)\n",
      "  Using cached kfp-0.1.24.tar.gz (74 kB)\n",
      "  Using cached kfp-0.1.23.1.tar.gz (73 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 has different version in metadata: '0.1.23'\u001b[0m\n",
      "  Using cached kfp-0.1.23.tar.gz (73 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 has different version in metadata: '0.1.22'\u001b[0m\n",
      "  Using cached kfp-0.1.19.tar.gz (67 kB)\n",
      "  Using cached kfp-0.1.16.tar.gz (148 kB)\n",
      "  Using cached kfp-0.1.11-py3-none-any.whl (212 kB)\n",
      "  Using cached kfp-0.1.11.tar.gz (130 kB)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.3.1)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.1b1 in ./.local/lib/python3.6/site-packages (from kfp) (1.1.2rc1)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.11.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.6.1->kfp) (1.11.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (45.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.8)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.1.2)\n",
      "Collecting kfp\n",
      "  Using cached kfp-1.1.2-py3-none-any.whl\n",
      "  Using cached kfp-1.1.1.tar.gz (162 kB)\n",
      "  Using cached kfp-1.1.0.tar.gz (172 kB)\n",
      "  Using cached kfp-1.0.4.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.3.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.1.tar.gz (116 kB)\n",
      "  Using cached kfp-1.0.0.tar.gz (116 kB)\n",
      "  Using cached kfp-0.5.1.tar.gz (119 kB)\n",
      "  Using cached kfp-0.5.0.tar.gz (118 kB)\n",
      "  Using cached kfp-0.4.0.tar.gz (118 kB)\n",
      "  Using cached kfp-0.3.0.tar.gz (117 kB)\n",
      "  Using cached kfp-0.2.5.tar.gz (116 kB)\n",
      "  Using cached kfp-0.2.4.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.4.tar.gz (115 kB)\n",
      "  Using cached kfp-0.2.2.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.2.tar.gz (114 kB)\n",
      "  Using cached kfp-0.2.1.tar.gz (112 kB)\n",
      "  Using cached kfp-0.2.0.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.40.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.39.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.38.tar.gz (112 kB)\n",
      "  Using cached kfp-0.1.37.tar.gz (111 kB)\n",
      "  Using cached kfp-0.1.36.tar.gz (111 kB)\n",
      "  Using cached kfp-0.1.35.tar.gz (105 kB)\n",
      "  Using cached kfp-0.1.34.tar.gz (104 kB)\n",
      "  Using cached kfp-0.1.33.tar.gz (103 kB)\n",
      "  Using cached kfp-0.1.32.2.tar.gz (101 kB)\n",
      "  Using cached kfp-0.1.32.1.tar.gz (100 kB)\n",
      "  Using cached kfp-0.1.32.tar.gz (102 kB)\n",
      "  Using cached kfp-0.1.31.2.tar.gz (99 kB)\n",
      "  Using cached kfp-0.1.31.1.tar.gz (97 kB)\n",
      "  Using cached kfp-0.1.31.tar.gz (94 kB)\n",
      "  Using cached kfp-0.1.30.tar.gz (88 kB)\n",
      "  Using cached kfp-0.1.29.tar.gz (88 kB)\n",
      "  Using cached kfp-0.1.27.tar.gz (80 kB)\n",
      "  Using cached kfp-0.1.26.tar.gz (81 kB)\n",
      "  Using cached kfp-0.1.25.tar.gz (76 kB)\n",
      "  Using cached kfp-0.1.24.tar.gz (74 kB)\n",
      "  Using cached kfp-0.1.23.1.tar.gz (73 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/8a/58/0a69e1adc106130eb893f49874f8c193caa6310e924fddc9a701db569431/kfp-0.1.23.1.tar.gz#sha256=fa460258d2b06837e29ff41fb78e56bab4f2e7af6fb5d288e19a5ff7e91c0de0 has different version in metadata: '0.1.23'\u001b[0m\n",
      "  Using cached kfp-0.1.23.tar.gz (73 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 (from https://pypi.org/simple/kfp/) (requires-python:>=3.5.3). Requested kfp from https://files.pythonhosted.org/packages/4d/e0/1c7085b53d3b48e42a2cac1a87300182c7d601deea11913a578d25eb91a4/kfp-0.1.23.tar.gz#sha256=22bb35bc178c2ad022a2cc113659568b78026632ff496cbf632313d90a86f124 has different version in metadata: '0.1.22'\u001b[0m\n",
      "  Using cached kfp-0.1.19.tar.gz (67 kB)\n",
      "  Using cached kfp-0.1.16.tar.gz (148 kB)\n",
      "  Using cached kfp-0.1.11-py3-none-any.whl (212 kB)\n",
      "  Using cached kfp-0.1.11.tar.gz (130 kB)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.1b1 in ./.local/lib/python3.6/site-packages (from kfp) (1.1.2rc1)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.3.1)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.11.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (45.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.6.1->kfp) (1.11.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.8)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n"
     ]
    }
   ],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "!pip3 install --user --upgrade kfp\n",
    "!pip3 install kfp --upgrade\n",
    "!pip3 install kfp --upgrade --user\n",
    "!pip3 install -U kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel before you proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after the pip install\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Components\n",
    "\n",
    "### Import the kfp and kfp.components packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp                  # the Pipelines SDK. \n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from kfp.dsl.types import Integer, GCSPath, String\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/g03-movie-success/data/out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a release experiment in the Kubeflow pipeline\n",
    "\n",
    "#### Kubeflow Pipeline requires having an Experiment before making a run. An experiment is a group of comparable runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Movie Analysis Pipeline'        # Name of the experiment in the UI\n",
    "BASE_IMAGE = \"tensorflow/tensorflow:latest-gpu-py3\"    # Base image used for components in the pipeline\n",
    "\n",
    "PROJECT_NAME = \"Kubeflow-mlops-pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the kfp.Client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/9cca4f23-2eda-48d5-b453-fd5c6ec7e723\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you run this command on a Jupyter notebook running on Kubeflow, you can\n",
    "# exclude the host parameter.\n",
    "# client = kfp.Client()\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section is were we start building the different Python function-based components. We will define the component's code as a standalone python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data for Exploratory Data Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_movies = \"gs://movie-success-bucket/data/tmdb_5000_movies.csv/tmdb_5000_movies.csv\"\n",
    "data_credits = \"gs://movie-success-bucket/data/tmdb_5000_credits.csv/tmdb_5000_credits.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_analysis(data_path):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function. \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'matplotlib']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'seaborn']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'wordcloud'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fsspec'])\n",
    "    import json\n",
    "    import gcsfs\n",
    "    import ast\n",
    "    from wordcloud import WordCloud\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    \n",
    "    #define a function that loads json columns in movies dataset\n",
    "    def load_tmdb_movies(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n",
    "        json_columns = ['genres', 'keywords', 'production_countries', 'production_companies', 'spoken_languages']\n",
    "        for column in json_columns:\n",
    "            df[column] = df[column].apply(json.loads)\n",
    "        return df\n",
    "    \n",
    "    #define a function that loads json columns in credits dataset\n",
    "    def load_tmdb_credits(path):\n",
    "        df = pd.read_csv(path)\n",
    "        json_columns = ['cast', 'crew']\n",
    "        for column in json_columns:\n",
    "            df[column] = df[column].apply(json.loads)\n",
    "        return df\n",
    "    \n",
    "    #load datasets\n",
    "    movies = load_tmdb_movies(\"gs://movie-success-bucket/data/tmdb_5000_movies.csv/tmdb_5000_movies.csv\")\n",
    "    credits = load_tmdb_credits(\"gs://movie-success-bucket/data/tmdb_5000_credits.csv/tmdb_5000_credits.csv\")\n",
    "    \n",
    "    # statistical info about numerical data\n",
    "    movies.describe()\n",
    "    \n",
    "    # info on variable types and filling factor\n",
    "    tab_info=pd.DataFrame(movies.dtypes).T.rename(index={0:'column type'})\n",
    "    tab_info=tab_info.append(pd.DataFrame(movies.isnull().sum()).T.rename(index={0:'null values'}))\n",
    "    tab_info=tab_info.append(pd.DataFrame(movies.isnull().sum()/movies.shape[0]*100).T.rename(index={0:'null values (%)'}))\n",
    "    tab_info\n",
    "    \n",
    "    \n",
    "    # Dealing with json Entries\n",
    "    # Some functions that will come handy during data wrangling\n",
    "    def extract_feature(x, field='name'):\n",
    "        \"\"\"\n",
    "        function is intended to extract values of a specified field\n",
    "        as a list...\n",
    "        \"\"\"\n",
    "        return [i[field] for i in x]\n",
    "    \n",
    "    def find_index_val(x, idx=0):\n",
    "        \"\"\"\n",
    "        Function would come in handy when\n",
    "        trying to find primary genre for example..\n",
    "        \"\"\"\n",
    "        return x[idx] if len(x) > 0 else np.NAN\n",
    "    \n",
    "    def cal_length(x):\n",
    "        \"\"\"\n",
    "        function to get the length of a value\n",
    "        \"\"\"\n",
    "        return len(x)\n",
    "    \n",
    "    def find_role(x, role='Director'):\n",
    "        \"\"\"\n",
    "        when the role is specified, function helps gets \n",
    "        the job of the specified role\n",
    "        \"\"\"\n",
    "        for i in x:\n",
    "            if i['job'] == role:\n",
    "                return i['name']\n",
    "        return np.NaN\n",
    "    \n",
    "    def find_animation(x):\n",
    "        \"\"\"\n",
    "        function helps to identify animated and \n",
    "        non-animated movies.\n",
    "        \"\"\"\n",
    "        if(len(x)==0):\n",
    "            return np.nan\n",
    "        elif('Animation' in x):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_lengths(x):\n",
    "        return len(x)\n",
    "    \n",
    "    \n",
    "    def safe_access(container, index_values):\n",
    "        result = container\n",
    "        try:\n",
    "            for idx in index_values:\n",
    "                result = result[idx]\n",
    "            return result\n",
    "        except IndexError or KeyError:\n",
    "            return pd.np.nan # return missing value rather than an error upon indexing/key failure\n",
    "        \n",
    "    \n",
    "    #get the genre list for each movie\n",
    "    movies['list_genres'] = movies['genres'].apply(extract_feature)\n",
    "    \n",
    "    \n",
    "    #Apply the functions to extract some useful features from the dataset\n",
    "    movies['primary_genre'] = movies['list_genres'].apply(find_index_val)\n",
    "\n",
    "    movies['list_keywords'] = movies['keywords'].apply(extract_feature)\n",
    "\n",
    "    movies['list_production_companies'] = movies['production_companies'].apply(extract_feature)\n",
    "\n",
    "    movies['num_production_companies'] = movies['list_production_companies'].apply(cal_length)\n",
    "\n",
    "    movies['list_productioin_countries'] = movies['production_countries'].apply(extract_feature)\n",
    "\n",
    "    movies['num_productioin_countries'] = movies['list_productioin_countries'].apply(cal_length)\n",
    "\n",
    "    movies['list_spoken_languages'] = movies['spoken_languages'].apply(extract_feature)\n",
    "\n",
    "    movies['num_spoken_languages'] = movies['list_spoken_languages'].apply(cal_length)\n",
    "\n",
    "    movies['animated'] = movies['list_genres'].apply(find_animation)\n",
    "    \n",
    "    new_movies = movies.copy()\n",
    "    \n",
    "    \n",
    "    # Movie Features\n",
    "    movie_features = ['id', 'original_title', 'budget', 'revenue', 'original_language', 'status', \n",
    "                  'release_date', 'overview', 'tagline', 'list_keywords', 'primary_genre', 'list_genres', \n",
    "                  'list_productioin_countries', 'num_production_companies', \n",
    "                  'num_productioin_countries', 'list_spoken_languages', 'num_spoken_languages', 'popularity', \n",
    "                  'vote_average', 'vote_count', 'runtime', 'animated']\n",
    "    \n",
    "    \n",
    "    # Copy the features for analysis\n",
    "    movie_final = movies[movie_features].copy()\n",
    "    movie_final.head()\n",
    "    \n",
    "    # Applying the extraction functions to the credits dataset\n",
    "    credits['movie_director'] = credits['crew'].apply(find_role, role='Director')\n",
    "    credits['crew_size'] = credits['crew'].apply(get_lengths)\n",
    "    credits['cast_size'] = credits['cast'].apply(get_lengths)\n",
    "\n",
    "    \n",
    "    # get credits features\n",
    "    credit_features = ['movie_id', 'title', 'movie_director', 'crew_size', 'cast_size']\n",
    "    \n",
    "    credit_final = credits[credit_features].copy()\n",
    "    \n",
    "    \n",
    "    credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['cast']], axis=1)\n",
    "    credits.apply(lambda row: [x.update({'movie_id': row['movie_id']}) for x in row['crew']], axis=1)\n",
    "    credits.apply(lambda row: [person.update({'order': order}) for order, person in enumerate(row['crew'])], axis=1)\n",
    "\n",
    "    # get list of cast\n",
    "    cast_list = []\n",
    "    credits[\"cast\"].apply(lambda x: cast_list.extend(x))\n",
    "    cast = pd.DataFrame(cast_list)\n",
    "    cast[\"type\"] = \"cast\"\n",
    "    \n",
    "    # get list of crew\n",
    "    crew_list = []\n",
    "    credits[\"crew\"].apply(lambda x: crew_list.extend(x))\n",
    "    crew = pd.DataFrame(crew_list)\n",
    "    crew[\"type\"] = \"crew\"\n",
    "    # list of cast and crew\n",
    "    people = pd.concat([cast, crew], ignore_index=True, sort=True)\n",
    "    \n",
    "    # Merge the movie_final and credit_final together\n",
    "    df = pd.merge(movie_final,credit_final, left_on='id', right_on='movie_id')\n",
    "    \n",
    "    \n",
    "    # Dealing with zero values, missing runtime data\n",
    "    # Replace all the zero values in the following columns with their respective mean values\n",
    "    df['revenue'] = df['revenue'].replace(0, np.nan).fillna(df['revenue'].mean())\n",
    "    df['budget'] = df['budget'].replace(0,np.nan).fillna(df['budget'].mean())\n",
    "    df['vote_count'] = df['vote_count'].replace(0, np.nan).fillna(df['vote_count'].mean())\n",
    "    df['vote_average'] = df['vote_average'].replace(0, np.nan).fillna(df['vote_average'].mean())\n",
    "    #fill missing 'runtime' values with mean.\n",
    "    df['runtime']=df['runtime'].replace(0, np.nan).fillna(df['runtime'].mean())\n",
    "    \n",
    "    \n",
    "    # Dealing with datetime values\n",
    "    df[\"release_date\"] = pd.to_datetime(df[\"release_date\"]) #convert release date to datetime\n",
    "    df[\"release_year\"] =df[\"release_date\"].dt.year #extract release year\n",
    "    df[\"release_month\"] = df[\"release_date\"].dt.month  #extract release month\n",
    "    df[\"release_quarter\"] = df[\"release_date\"].dt.quarter #extract release quarter\n",
    "\n",
    "    # copy movies\n",
    "    new_movies.to_pickle(f'{data_path}/movies.pkl')\n",
    "\n",
    "    # Save Dataframe using the pickle extension\n",
    "    df.to_pickle(f'{data_path}/preprocessed-data-analysis.pkl')\n",
    "    print(\"Preprocessing data for Exploratory data analysis Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_data_analysis(data_path):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function. \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'matplotlib']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'seaborn']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'wordcloud'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'urllib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fsspec'])\n",
    "    \n",
    "#     pip install urllib3\n",
    "    import json\n",
    "    import ast\n",
    "    from wordcloud import WordCloud\n",
    "    import warnings\n",
    "    import gcsfs\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import json\n",
    "    import pickle\n",
    "    import urllib\n",
    "    \n",
    "    \n",
    "    # Read the preprocessed pickle data file\n",
    "    df = pd.read_pickle(f'{data_path}/preprocessed-data-analysis.pkl')\n",
    "    \n",
    "    # get movies\n",
    "    movies = pd.read_pickle(f'{data_path}/movies.pkl')\n",
    "    \n",
    "    df.head()\n",
    "    \n",
    "    # DATA ANALYSIS AND VISUALIZATION\n",
    "    # General Distirbution of Numerical Values\n",
    "    \n",
    "    #Initiate KDE subplots for numerical columns\n",
    "    fig, axarr = plt.subplots(4, 2, figsize=(15, 20))\n",
    "    sns.kdeplot(df[\"budget\"], ax=axarr[0][0])\n",
    "    axarr[0][0].xaxis.set_ticks(np.arange(0, 4.25e8, 0.50e8))\n",
    "\n",
    "    sns.kdeplot(df[\"revenue\"], ax=axarr[0][1])\n",
    "    axarr[0][1].xaxis.set_ticks(np.arange(0, 3e9, 0.50e9))\n",
    "    sns.kdeplot(df[\"runtime\"], ax=axarr[1][0])\n",
    "\n",
    "    sns.kdeplot(df[\"popularity\"], ax=axarr[1][1])\n",
    "    axarr[1][1].xaxis.set_ticks(np.arange(0, 900, 50))\n",
    "\n",
    "    sns.kdeplot(df[\"vote_average\"], ax=axarr[2][0])\n",
    "    axarr[2][0].xaxis.set_ticks(np.arange(0, 11, 1))\n",
    "\n",
    "    sns.kdeplot(df[\"vote_count\"], ax=axarr[2][1])\n",
    "    sns.kdeplot(df[\"release_year\"], ax=axarr[3][0])\n",
    "\n",
    "    axarr[3][1].axis(\"off\")\n",
    "    \n",
    "    #set subplot titles\n",
    "    axarr[0][0].set_title('Budget Distribution', fontsize = 24)\n",
    "    axarr[0][1].set_title('Revenue Distribution', fontsize = 24)\n",
    "    axarr[1][0].set_title('Runtime Distribution', fontsize = 24)\n",
    "    axarr[1][1].set_title('Popularity Distribution', fontsize = 24)\n",
    "    axarr[2][0].set_title('Average vote Distribution', fontsize = 24)\n",
    "    axarr[2][1].set_title('Vote count Distribution', fontsize = 24)\n",
    "    axarr[3][0].set_title('Release Year Distribution', fontsize = 24)\n",
    "\n",
    "    axarr[0][0].set_xlabel('Budget ($)', fontsize = 12)\n",
    "    axarr[0][1].set_xlabel('Revenue ($)', fontsize = 12)\n",
    "    axarr[1][0].set_xlabel('Runtime (min)', fontsize = 12)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    # Innitiate scatter plots with correlation lines\n",
    "    fig, axarr = plt.subplots(3, 2, figsize=(15, 20))\n",
    "    p_color = dict(color=\"C0\")\n",
    "    l_color = dict(color=\"C1\")\n",
    "    sns.regplot(x=\"budget\", y=\"revenue\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[0][0])\n",
    "    sns.regplot(x=\"runtime\", y=\"revenue\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[0][1])\n",
    "    sns.regplot(x=\"release_year\", y=\"revenue\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[1][0])\n",
    "    sns.regplot(x=\"popularity\", y=\"revenue\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[1][1])\n",
    "    sns.regplot(x=\"vote_average\", y=\"revenue\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[2][0])\n",
    "    sns.regplot(x=\"vote_count\", y=\"revenue\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[2][1])\n",
    "\n",
    "    axarr[0][0].set_xlabel('Budget ($)', fontsize = 12)\n",
    "    axarr[0][0].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "    axarr[0][1].set_xlabel('Runtime (min)', fontsize = 12)\n",
    "    axarr[1][0].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "    axarr[2][0].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "    axarr[2][1].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "\n",
    "    #Set plot titles\n",
    "    axarr[0][0].set_title('Revenue vs Budget', fontsize = 25)\n",
    "    axarr[0][1].set_title('Revenue vs Runtime', fontsize = 25)\n",
    "    axarr[1][0].set_title('Revenue vs Release Year', fontsize = 25)\n",
    "    axarr[1][1].set_title('Revenue vs Popularity', fontsize = 25)\n",
    "    axarr[2][0].set_title('Revenue vs Average Vote', fontsize = 25)\n",
    "    axarr[2][1].set_title('Revenue vs Vote Count', fontsize = 25)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    #computes profit or loss from budget and revenue\n",
    "    df['profit'] = df['revenue'] - df['budget'] \n",
    "    df['profit_rate'] = df['profit'] / df['budget']\n",
    "    \n",
    "    fig, axarr = plt.subplots(3, 2, figsize=(15, 15))\n",
    "    p_color = dict(color=\"C0\")\n",
    "    l_color = dict(color=\"C1\")\n",
    "    sns.regplot(x=\"profit\", y=\"budget\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[0][0])\n",
    "    sns.regplot(x=\"runtime\", y=\"budget\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[0][1])\n",
    "    sns.regplot(x=\"release_year\", y=\"budget\", data = df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[1][0])\n",
    "    sns.regplot(x=\"popularity\", y=\"budget\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[1][1])\n",
    "    sns.regplot(x=\"vote_average\", y=\"budget\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[2][0])\n",
    "    sns.regplot(x=\"vote_count\", y=\"budget\", data=df, fit_reg=True, scatter_kws=p_color, line_kws=l_color, ax=axarr[2][1])\n",
    "\n",
    "    axarr[0][0].set_ylabel('Budget ($)', fontsize = 12)\n",
    "    axarr[0][0].set_xlabel('Profit ($)', fontsize = 12)\n",
    "    axarr[0][1].set_xlabel('Runtime (min)', fontsize = 12)\n",
    "    axarr[1][0].set_ylabel('Budget ($)', fontsize = 12)\n",
    "    axarr[2][0].set_ylabel('Budget ($)', fontsize = 12)\n",
    "\n",
    "    #Set title for each plot\n",
    "    axarr[0][0].set_title('Budget vs Profit', fontsize = 20)\n",
    "    axarr[0][1].set_title('Budget vs Runtime', fontsize = 20)\n",
    "    axarr[1][0].set_title('Budget vs Release Year', fontsize = 20)\n",
    "    axarr[1][1].set_title('Budget vs Popularity', fontsize = 20)\n",
    "    axarr[2][0].set_title('Budget vs Average Vote', fontsize = 20)\n",
    "    axarr[2][1].set_title('Budget vs Vote Count', fontsize = 20)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    # Exploring Budget and Revenue with Respect to Datetime\n",
    "    #computes total revenue, profit, budget per release year\n",
    "    revenues = df.groupby('release_year')['revenue'].sum() \n",
    "    budgets = df.groupby('release_year')['budget'].sum()\n",
    "    profits = df.groupby('release_year')['profit'].sum()\n",
    "    rate = df.groupby('release_year')['profit_rate'].mean()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    x1 = df['release_year']\n",
    "    y1 = revenues\n",
    "    # plotting the revenue points \n",
    "    plt.plot(revenues, label = \"Revenues\")\n",
    "    # line 2 points\n",
    "    x2 = df['release_year']\n",
    "    y2 = budgets\n",
    "    # plotting the budgets points \n",
    "    plt.plot(budgets, label = \"Budgets\")\n",
    "    x3 = df['release_year']\n",
    "    y3 = profits\n",
    "    # plotting the profits points\n",
    "    plt.plot(profits, label = \"Profits\")\n",
    "    x4 = df['release_year']\n",
    "    y4 = rate\n",
    "    plt.plot(rate,label='profit rate')\n",
    "    \n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    # Set the y axis label of the current axis.\n",
    "    plt.ylabel('Amount ($)')\n",
    "    # Set a title of the current axes.\n",
    "    plt.title('Trends in Revenue, Budget and Profit over the Years', fontsize =25)\n",
    "    # show a legend on the plot\n",
    "    plt.legend(loc='upper left', fontsize = 15)\n",
    "    # Display a figure.\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    df['release_year'].max()\n",
    "    \n",
    "    last_10_years = df[df['release_year']>2007]\n",
    "    \n",
    "    \n",
    "    revenues = last_10_years.groupby('release_year')['revenue'].sum()\n",
    "    budgets = last_10_years.groupby('release_year')['budget'].sum()\n",
    "    profits = last_10_years.groupby('release_year')['profit'].sum()\n",
    "    rate = last_10_years.groupby('release_year')['profit_rate'].mean()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(14,7))\n",
    "    x1 = df['release_year']\n",
    "    y1 = revenues\n",
    "    # plotting the revenue points \n",
    "    plt.plot(revenues, label = \"Revenues\")\n",
    "    # line 2 points\n",
    "    x2 = df['release_year']\n",
    "    y2 = budgets\n",
    "    # plotting the budgets points \n",
    "    plt.plot(budgets, label = \"Budgets\")\n",
    "    x3 = df['release_year']\n",
    "    y3 = profits\n",
    "    # plotting the profits points\n",
    "    plt.plot(profits, label = \"Profits\")\n",
    "    x4 = df['release_year']\n",
    "    y4 = rate\n",
    "    plt.plot(rate,label='profit rate')\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    # Set the y axis label of the current axis.\n",
    "    plt.ylabel('Amount ($)')\n",
    "    # Set a title of the current axes.\n",
    "    plt.title('Trends in Revenue, Budget and Profit over the last 10 Years', fontsize =25)\n",
    "    # show a legend on the plot\n",
    "    plt.legend(loc='upper left', fontsize = 14)\n",
    "    # Display a figure.\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    # How many movies have grossed over $1 billion for each which year?\n",
    "    billi_movies = df[df['revenue']>1000000000].sort_values('revenue', ascending =False)\n",
    "    \n",
    "    \n",
    "    fig =sns.catplot(x='release_year', data=billi_movies, kind=\"count\", palette = 'prism')\n",
    "    fig.set_xticklabels(rotation=45)\n",
    "    plt.title('Number of Movies that Grossed more than $1 Billion', fontsize = 20)\n",
    "    plt.gcf().set_size_inches(10, 4)\n",
    "    \n",
    "    \n",
    "    #compute total and avg revenue per month, quarter\n",
    "    monthly_revenue = df.groupby('release_month')['revenue'].sum().to_frame().reset_index()\n",
    "    avg_monthly_revenue =  df.groupby('release_month')['revenue'].mean().to_frame().reset_index()\n",
    "    quarterly_revenue = df.groupby('release_quarter')['revenue'].sum().to_frame().reset_index()\n",
    "    avg_quarterly_revenue =  df.groupby('release_quarter')['revenue'].mean().to_frame().reset_index()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Initiate subplots for different distribution of revenue by relese month\n",
    "    fig, axarr = plt.subplots(3,2, figsize=(15, 15))\n",
    "    sns.barplot(x='release_month', y = 'revenue', data = monthly_revenue, ax = axarr[1][0], palette = 'prism')\n",
    "    sns.countplot(df[\"release_month\"], ax=axarr[0][0], palette='prism')\n",
    "    sns.barplot(x='release_month', y = 'revenue', data = avg_monthly_revenue, ax = axarr[2][0], palette = 'prism')\n",
    "    sns.barplot(x='release_quarter', y='revenue', data =avg_quarterly_revenue,ax =axarr[2][1],palette ='magma')\n",
    "    sns.countplot(df['release_quarter'], ax=axarr[0][1],palette ='magma')\n",
    "    sns.barplot(x='release_quarter', y = 'revenue', data = quarterly_revenue, ax = axarr[1][1],palette ='magma')\n",
    "\n",
    "    #set plot titles\n",
    "\n",
    "    axarr[2][0].set_title('Average Monthly Revenue', fontsize = 20)\n",
    "    axarr[0][0].set_title('Number of Movies Released Per Month', fontsize = 20)\n",
    "    axarr[1][0].set_title('Total Monthly Revenue', fontsize = 20)\n",
    "    axarr[0][1].set_title('Number of Movies Released Per Quarter',fontsize =20)\n",
    "    axarr[1][1].set_title('Total Quarterly Revenue',fontsize = 20)\n",
    "    axarr[2][1].set_title('Average Quartely Revenue',fontsize = 20)\n",
    "\n",
    "    axarr[1][0].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "    axarr[2][0].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "\n",
    "    #add customized labels to plots\n",
    "    labels = ['Jan', 'Feb', 'Mar','April', 'May', 'Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    x =[axarr[0][0], axarr[1][0],axarr[2][0]]\n",
    "    for a in x:\n",
    "        a.set_xticklabels(labels,fontsize = 14)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Initiate subplots for different distribution of revenue by relese month\n",
    "    fig, axarr = plt.subplots(2,1, figsize=(15, 15))\n",
    "    sns.boxplot(x = 'release_month', y = 'revenue', data = df, ax = axarr[0], palette ='prism')\n",
    "    sns.boxplot(x = 'release_quarter', y = 'revenue', data = df, ax = axarr[1],palette ='rainbow')\n",
    "\n",
    "    axarr[0].set_title('Revenue vs Release Month', fontsize = 25)\n",
    "    axarr[1].set_title('Revenue vs Release Quarter',fontsize = 25)\n",
    "    axarr[0].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "    axarr[1].set_ylabel('Revenue ($)', fontsize = 12)\n",
    "\n",
    "    axarr[0].set_xticklabels(labels,fontsize = 14)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exploring Movies\n",
    "    # Which movies have the highest ratings?\n",
    "    C= movies['vote_average'].mean()\n",
    "    m= movies['vote_count'].quantile(0.9)\n",
    "    \n",
    "    \n",
    "    q_movies = df.copy().loc[df['vote_count'] >= m]\n",
    "    q_movies.shape\n",
    "\n",
    "    def weighted_rating(x, m=m, C=C):\n",
    "        v = x['vote_count']\n",
    "        R = x['vote_average']\n",
    "        # Calculation based on the IMDB formula\n",
    "        return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "    \n",
    "    \n",
    "    # Define a new feature 'score' and calculate its value with `weighted_rating()`\n",
    "    q_movies['score'] = q_movies.apply(weighted_rating, axis=1)\n",
    "    \n",
    "    \n",
    "    #Sort movies based on score calculated above\n",
    "    q_movies = q_movies.sort_values('score', ascending=False)\n",
    "\n",
    "    #Print the top 15 movies according to IMDB rating\n",
    "    q_movies[['original_title','movie_director', 'vote_count', 'vote_average', 'score','release_year']].head(15)\n",
    "\n",
    "    \n",
    "    # Most popular and least popular movies\n",
    "    pop= df.sort_values('popularity', ascending=False)\n",
    "    pop_10 = pop.head(10)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12,10))\n",
    "\n",
    "    sns.barplot(y = 'original_title',x = 'popularity',data= pop_10,palette = 'prism', hue='movie_director', dodge = False)\n",
    "    #plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Popularity\", fontsize = 16)\n",
    "    plt.ylabel(\"\") \n",
    "    plt.title(\"Most Popular Movies\", fontsize = 25)\n",
    "    plt.legend(title ='Movie Director',fontsize = 15).get_title().set_fontsize(18)\n",
    "    plt.yticks(fontsize= 15)\n",
    "    \n",
    "    \n",
    "    least_10 = pop.tail(10)\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    least_10.index = least_10.original_title.values\n",
    "    chart = least_10['popularity'].sort_values(ascending =True).plot.barh(color=sns.color_palette(\"prism\",10), fontsize = 15)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    chart.set_xlabel('Popularity')\n",
    "    chart.set_ylabel('Movie')\n",
    "    chart.set_title('Least Popular Movies', fontsize = 25)\n",
    "\n",
    "    \n",
    "    # Big Budget Movies\n",
    "    big_budget = df.sort_values('budget', ascending = False).iloc[:10]\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.barplot(x = 'budget',\n",
    "                y = 'title',\n",
    "                palette='magma',\n",
    "                data = big_budget,\n",
    "               hue= 'movie_director',\n",
    "               dodge = False)\n",
    "    plt.title('Big Budget Movies', fontsize = 20)\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.xticks(fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    plt.legend(title ='Movie Director',fontsize = 14).get_title().set_fontsize(18)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # How many movies made profit or loss?\n",
    "    \n",
    "    \n",
    "    print(\"Exploratory Data Analysis Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"movie-success-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>...</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...</td>\n",
       "      <td>[{\"credit_id\": \"52fe48009251416c750aca23\", \"de...</td>\n",
       "      <td>237000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"name\": \"Ingenious Film Partners\", \"id\": 289...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2787965087</td>\n",
       "      <td>162.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>[{\"cast_id\": 4, \"character\": \"Captain Jack Spa...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...</td>\n",
       "      <td>300000000</td>\n",
       "      <td>[{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...</td>\n",
       "      <td>http://disney.go.com/disneypictures/pirates/</td>\n",
       "      <td>[{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>[{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2007-05-19</td>\n",
       "      <td>961000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>At the end of the world, the adventure begins.</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                     title  \\\n",
       "0           0  19995                                    Avatar   \n",
       "1           1    285  Pirates of the Caribbean: At World's End   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...   \n",
       "1  [{\"cast_id\": 4, \"character\": \"Captain Jack Spa...   \n",
       "\n",
       "                                                crew     budget  \\\n",
       "0  [{\"credit_id\": \"52fe48009251416c750aca23\", \"de...  237000000   \n",
       "1  [{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...  300000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "1  [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...   \n",
       "\n",
       "                                       homepage  \\\n",
       "0                   http://www.avatarmovie.com/   \n",
       "1  http://disney.go.com/disneypictures/pirates/   \n",
       "\n",
       "                                            keywords original_language  ...  \\\n",
       "0  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...                en  ...   \n",
       "1  [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...                en  ...   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{\"name\": \"Ingenious Film Partners\", \"id\": 289...   \n",
       "1  [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...   \n",
       "\n",
       "                                production_countries  release_date  \\\n",
       "0  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...    2009-12-10   \n",
       "1  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...    2007-05-19   \n",
       "\n",
       "      revenue runtime                                   spoken_languages  \\\n",
       "0  2787965087   162.0  [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...   \n",
       "1   961000000   169.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]   \n",
       "\n",
       "     status                                         tagline vote_average  \\\n",
       "0  Released                     Enter the World of Pandora.          7.2   \n",
       "1  Released  At the end of the world, the adventure begins.          6.9   \n",
       "\n",
       "  vote_count  \n",
       "0      11800  \n",
       "1       4500  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"gs://{}/data/merged_movies_dataset.csv\".format(bucket))\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Python function-based components\n",
    "\n",
    "### Define your component's code as a standalone python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_modeling(data_path):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fsspec'])\n",
    "    import ast\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import gcsfs\n",
    "    import numpy as np\n",
    "    from ast import literal_eval\n",
    "    from pandas import Series, DataFrame,read_csv\n",
    "    import pickle\n",
    "    \n",
    "    # get bucket name\n",
    "#     bucket = \"movie-success-bucket\"\n",
    "\n",
    "        #define a function that loads json columns in movies dataset\n",
    "    def load_tmdb_movies(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n",
    "        json_columns = ['genres', 'keywords', 'production_countries', 'production_companies', 'spoken_languages']\n",
    "        for column in json_columns:\n",
    "            df[column] = df[column].apply(json.loads)\n",
    "        return df\n",
    "    \n",
    "    #define a function that loads json columns in credits dataset\n",
    "    def load_tmdb_credits(path):\n",
    "        df = pd.read_csv(path)\n",
    "        json_columns = ['cast', 'crew']\n",
    "        for column in json_columns:\n",
    "            df[column] = df[column].apply(json.loads)\n",
    "        return df\n",
    "    \n",
    "    #load datasets\n",
    "    df_movies = load_tmdb_movies(\"gs://movie-success-bucket/data/tmdb_5000_movies.csv/tmdb_5000_movies.csv\")\n",
    "    df_credits = load_tmdb_credits(\"gs://movie-success-bucket/data/tmdb_5000_credits.csv/tmdb_5000_credits.csv\")\n",
    "    \n",
    "    df_credits.rename(columns = {'movie_id':'id'}, inplace = True)\n",
    "    \n",
    "    df_movies.drop('original_title', axis = 1, inplace = True)\n",
    "    \n",
    "    data = pd.merge(df_credits, df_movies, on = ['id','title'])\n",
    "    \n",
    "    \n",
    "    # Handling the Json Columns\n",
    "    # Applying the literal_eval function of ast on all the json columns\n",
    "#     json_cols = ['cast','crew','genres','keywords','production_companies','production_countries','spoken_languages']\n",
    "#     for col in json_cols:\n",
    "#         data[col] = data[col].apply(literal_eval)\n",
    "#         data[col] = data[col].apply(json.loads)\n",
    "        \n",
    "        \n",
    "    # Helper Functions for the same\n",
    "    # function to get the names of the movies genre\n",
    "    def get_genre(x):\n",
    "        if(isinstance(x, list)):\n",
    "            genre = [i['name'] for i in x]\n",
    "    \n",
    "        return genre\n",
    "\n",
    "    # function to get the jobs of the crew members \n",
    "    def get_jobs(x):\n",
    "        if(isinstance(x, list)):\n",
    "            jobs = [i['job'] for i in x]\n",
    "        return jobs\n",
    "\n",
    "    # function to get the target/label (Animation == 1 / Not_Animation == 0)\n",
    "    def get_labels(x):\n",
    "        if(len(x)==0):\n",
    "            return np.nan\n",
    "        elif('Animation' in x):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Get percentage of voice artists among total cast\n",
    "    def get_characternames(x):\n",
    "        if(isinstance(x, list)):\n",
    "            chr_name = [i['character'] for i in x]\n",
    "            countc = 0\n",
    "            for j in chr_name:\n",
    "                if('(voice)' in j):\n",
    "                    countc += 1\n",
    "            if(len(chr_name)!=0):\n",
    "                return (countc/len(chr_name))\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # function to get crew memebers whose jobs are Costume Design\n",
    "    def get_costume_labels(x):\n",
    "        if 'Costume Design' in x:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    # function to get the genre department with the Lighting role\n",
    "    def get_genre_cd(x):\n",
    "        if(isinstance(x, list)):\n",
    "            dept = [i['department'] for i in x]\n",
    "        if 'Lighting' in dept:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    # Applying the above functions \n",
    "    data['genres'] = data['genres'].apply(get_genre)\n",
    "    data['crew_jobs'] = data['crew'].apply(get_jobs)\n",
    "    data['percent_of_voice_artists'] = data['cast'].apply(get_characternames)\n",
    "    data['labels'] = data['genres'].apply(get_labels)\n",
    "    \n",
    "    # Rounding off the percentage to 3 decimal places\n",
    "    for x in range(0,len(data['percent_of_voice_artists'])):\n",
    "        data['percent_of_voice_artists'][x] = np.round(data['percent_of_voice_artists'][x],3)\n",
    "        \n",
    "    # number of Labels missing / Null values  \n",
    "    data.labels.isna().sum()\n",
    "    \n",
    "    \n",
    "    # dealing with Labels missing values\n",
    "    idxsc = data[((data.labels != 1) & (data.labels != 0))].index\n",
    "    data.drop(idxsc, inplace = True)\n",
    "    data.reset_index(drop= True, inplace= True)\n",
    "    \n",
    "    # checking for dataset Features with missing values\n",
    "    data.isna().sum()\n",
    "    \n",
    "    # check the number of animated and non_animated movies\n",
    "    AnimatedMoviesCount = np.sum(data['labels'] == 1)\n",
    "    NotAnimatedMoviesCount = np.sum(data['labels'] == 0)\n",
    "\n",
    "#     print(\"Number of Animated Movies are: \", AnimatedMoviesCount)\n",
    "#     print(\"Number of Not Animated Movies are: \", NotAnimatedMoviesCount)\n",
    "\n",
    "    # Apply the get_costume_labels function\n",
    "    data['costume'] = data['crew_jobs'].apply(get_costume_labels)\n",
    "    \n",
    "    data.costume.value_counts()\n",
    "    \n",
    "    # Apply get_genre_cd function\n",
    "    data['lighting_dept'] = data['crew'].apply(get_genre_cd)\n",
    "\n",
    "    data.lighting_dept.value_counts()\n",
    "    \n",
    "    # Taking into account only those movies having atleast 7 crew members\n",
    "    # So as to handle the quality of training data Tested for multiple values, but 7 yielded best result\n",
    "    idx=[]\n",
    "    for x in range(0,data.shape[0]):\n",
    "        if len(data.crew_jobs[x])>7:\n",
    "            idx.append(x)\n",
    "    print(\"Number of Movies with more than 7 crew members: \",str(len(idx)))\n",
    "\n",
    "    df = data.iloc[idx,:]\n",
    "    \n",
    "    \n",
    "    # Get the number of animated and non_animated movies\n",
    "    AnimatedMoviesCount2 = np.sum(df['labels'] == 1)\n",
    "    NotAnimatedMoviesCount2 = np.sum(df['labels'] == 0)\n",
    "    \n",
    "    print(\"Number of Animated Movies are: \", AnimatedMoviesCount2)\n",
    "    print(\"Number of Not Animated Movies are: \", NotAnimatedMoviesCount2)\n",
    "    \n",
    "    \n",
    "    # Converting 'crew_jobs' from list to string (in lower form) via join function\n",
    "    def join_strings(x):\n",
    "        return \", \".join(x)\n",
    "\n",
    "    def str_lower(x):\n",
    "        return x.lower()\n",
    "\n",
    "    df['crew_jobs'] = df['crew_jobs'].apply(join_strings)\n",
    "    df['crew_jobs'] = df['crew_jobs'].apply(str_lower)\n",
    "    \n",
    "    # get the number of labels\n",
    "    df['labels'].value_counts() \n",
    "    \n",
    "    # Save Dataframe using the pickle extension\n",
    "    df.to_pickle(f'{data_path}/preprocessed-data-model.pkl')\n",
    "    print(\"Preprocessing Done\")\n",
    "    \n",
    "    #Save preprocessed data\n",
    "#     df.to_csv(\"data/preprocessed\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Movies with more than 7 crew members:  3653\n",
      "Number of Animated Movies are:  193\n",
      "Number of Not Animated Movies are:  3460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Done\n"
     ]
    }
   ],
   "source": [
    "data_path = \"gs://{}/data/merged_movies_dataset.csv\".format(bucket)\n",
    "\n",
    "preprocess_data_modeling(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed data to google cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BucketNotFoundException: 404 gs://-success-bucket bucket does not exist.\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/preprocessed gs://${bucket}/data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where preprocessed data is stored\n",
    "in_dir = \"gs://{}/data/preprocessed\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def model_training(data_path, model_file):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'imblearn']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorboard'])  \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'IPython'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz'])\n",
    "    \n",
    "    import pandas as pd\n",
    "    import en_core_web_sm\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import imblearn\n",
    "    import spacy\n",
    "    from spacy.lang.en import STOP_WORDS\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    \n",
    "    # get data\n",
    "#     df = pd.read_csv(\"gs://movie-success-bucket/data/preprocessed\")\n",
    "    \n",
    "    #load the transformed data\n",
    "    df = pd.read_pickle(f'{data_path}/preprocessed-data-model.pkl')\n",
    "    df.head()\n",
    "    \n",
    "    # Get the features and labels\n",
    "    X = df['crew_jobs']\n",
    "    y = df['labels']\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=53)\n",
    "    \n",
    "    # function to output our scores \n",
    "    def score_output(y_test, y_pred):\n",
    "        print(metrics.confusion_matrix(y_test, y_pred))\n",
    "        print(metrics.classification_report(y_test, y_pred))\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('The Accuracy on The Test Set is: %s' % accuracy)\n",
    "        \n",
    "    # model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "#     nlp = en_core_web_sm.load()\n",
    "    \n",
    "    # instantiate stopwords to use\n",
    "    stop_words_str = \" \".join(STOP_WORDS)\n",
    "    stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "    additional_words = ['editor', 'director', 'producer', 'writer', 'assistant', 'sound']\n",
    "\n",
    "    for word in additional_words:\n",
    "        stop_words_lemma = stop_words_lemma.union({word})\n",
    "        \n",
    "    # define the lemmatizer function\n",
    "    def lemmatizer(text):\n",
    "        return [word.lemma_ for word in nlp(text)]\n",
    "    \n",
    "    # Without Stop Words\n",
    "    bow = TfidfVectorizer(ngram_range = (1,1))\n",
    "\n",
    "    model = Pipeline([('bag_of_words', bow),('classifier', SVC())])\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    print(\"Without Stop Words\")\n",
    "    print('Training accuracy: {}'.format(model.score(X_train,y_train)))\n",
    "    y_pred = model.predict(X_test)\n",
    "    score_output(y_test, y_pred)\n",
    "    \n",
    "    # output the splitted data file to path \n",
    "    np.savez_compressed(f'{data_path}/train-test-data.npz', \n",
    "                       X_train=X_train,\n",
    "                       X_test=X_test,\n",
    "                       y_train=y_train,\n",
    "                       y_test=y_test,  allow_pickle=True)\n",
    "    \n",
    "    #Save the model as a pickle file.\n",
    "    with open(f'{data_path}/{model_file}', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "    # Save the classifier model to the designated \n",
    "#     with open(f'{data_path}/{classifier_file}', 'wb') as file:\n",
    "#         pickle.dump(classifier, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = model_training(out_dir, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export saved model to google cloud storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {out_dir}/model gs://${bucket}/{out_dir}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def model_validation(data_path, model_file) -> NamedTuple(\n",
    "    'ModelvalidationOutputs',\n",
    "    [\n",
    "      ('recall', float),\n",
    "      ('accuracy', float),\n",
    "      ('precision', float),\n",
    "      ('f1score', float),\n",
    "      ('mlpipeline_metrics', 'Metrics')\n",
    "    ]):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib']) \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import pickle\n",
    "    from sklearn.metrics import classification_report, recall_score, accuracy_score,precision_score, f1_score, confusion_matrix\n",
    "    \n",
    "    \n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/{model_file}','rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    # load the transformed data\n",
    "    train_test_data = np.load(f'{data_path}/train-test-data.npz', allow_pickle=True)\n",
    "    X_train = train_test_data['X_train']\n",
    "    X_test  = train_test_data['X_test']\n",
    "    y_train = train_test_data['y_train']\n",
    "    y_test  = train_test_data['y_test']\n",
    "    \n",
    "        \n",
    "    # write out metrics\n",
    "    accuracy = model.score(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # Model Evaluation\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    f1score = f1_score(y_test,y_pred)\n",
    "        \n",
    "    # Export metrics\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "        'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "        'numberValue':  accuracy, # The value of the metric. Must be a numeric value.\n",
    "        'format': \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "      },{\n",
    "        'name': 'recall-score',\n",
    "        'numberValue': recall,\n",
    "        'format': \"PERCENTAGE\",\n",
    "      },{\n",
    "        'name': 'precision-score',\n",
    "        'numberValue': precision,\n",
    "        'format': \"PERCENTAGE\",\n",
    "      },{\n",
    "        'name': 'f1score',\n",
    "        'numberValue': f1score,\n",
    "        'format': \"PERCENTAGE\",\n",
    "      }]}\n",
    "    \n",
    "    # Classification Report table\n",
    "    report = classification_report(y_test,y_pred)\n",
    "    print(report)\n",
    "    \n",
    "    # The Report file\n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\"Report: {} \".format(report))\n",
    "        \n",
    "    #output the splitted data file to path\n",
    "    np.savez_compressed(f'{data_path}/validated-data.npz', \n",
    "                       X_test=X_test,\n",
    "                       y_test=y_test,\n",
    "                       y_pred=y_pred)\n",
    "\n",
    "    # Save y_pred and y_test as pickle files\n",
    "    pickle.dump(y_pred, open(f'{data_path}/y_pred.pkl','wb'))\n",
    "    pickle.dump(y_test, open(f'{data_path}/y_test.pkl','wb'))\n",
    "    \n",
    "    # Save the classifier model to the designated \n",
    "    with open(f'{data_path}/{model_file}', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "    \n",
    "        \n",
    "    with open(f'{data_path}/classifier_result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {},\\n\\nActual: {} \".format(y_pred, y_test))\n",
    "        \n",
    "    from collections import namedtuple\n",
    "    model_eval_output = namedtuple(\n",
    "        'ModelvalidationOutputs',\n",
    "        ['accuracy', 'recall', 'precision', 'f1score',  'mlpipeline_metrics']) \n",
    "    return model_eval_output(accuracy, recall, precision, f1score,  json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_validation(out_dir, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def confusion_matrix(data_path, model_file):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'matplotlib']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib']) \n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "    \n",
    "    \n",
    "     # Load the saved classifier model\n",
    "    with open(f'{data_path}/{model_file}', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    # Load the y_pred data file\n",
    "    pickle_in = open(f'{data_path}/y_pred.pkl',\"rb\")\n",
    "    y_pred = pickle.load(pickle_in)\n",
    "    \n",
    "    # Load the y_test data file\n",
    "    pickle_ = open(f'{data_path}/y_test.pkl',\"rb\")\n",
    "    y_test = pickle.load(pickle_)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    matrix = confusion_matrix(y_test.reshape(-1,1), y_pred)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>status</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>release_year</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['Sam Worthington', 'Zoe Saldana', 'Sigourney ...</td>\n",
       "      <td>['James Cameron']</td>\n",
       "      <td>237000000</td>\n",
       "      <td>['Action', 'Adventure', 'Fantasy', 'Science Fi...</td>\n",
       "      <td>['culture clash', 'future', 'space war', 'spac...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>['Ingenious Film Partners', 'Twentieth Century...</td>\n",
       "      <td>['United States of America', 'United Kingdom']</td>\n",
       "      <td>2787965087</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['Johnny Depp', 'Orlando Bloom', 'Keira Knight...</td>\n",
       "      <td>['Gore Verbinski']</td>\n",
       "      <td>300000000</td>\n",
       "      <td>['Adventure', 'Fantasy', 'Action']</td>\n",
       "      <td>['ocean', 'drug abuse', 'exotic island', 'east...</td>\n",
       "      <td>en</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>139.082615</td>\n",
       "      <td>['Walt Disney Pictures', 'Jerry Bruckheimer Fi...</td>\n",
       "      <td>['United States of America']</td>\n",
       "      <td>961000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>Released</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4500</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               cast  \\\n",
       "0           0  ['Sam Worthington', 'Zoe Saldana', 'Sigourney ...   \n",
       "1           1  ['Johnny Depp', 'Orlando Bloom', 'Keira Knight...   \n",
       "\n",
       "                 crew     budget  \\\n",
       "0   ['James Cameron']  237000000   \n",
       "1  ['Gore Verbinski']  300000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  ['Action', 'Adventure', 'Fantasy', 'Science Fi...   \n",
       "1                 ['Adventure', 'Fantasy', 'Action']   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  ['culture clash', 'future', 'space war', 'spac...                en   \n",
       "1  ['ocean', 'drug abuse', 'exotic island', 'east...                en   \n",
       "\n",
       "                             original_title  popularity  \\\n",
       "0                                    Avatar  150.437577   \n",
       "1  Pirates of the Caribbean: At World's End  139.082615   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  ['Ingenious Film Partners', 'Twentieth Century...   \n",
       "1  ['Walt Disney Pictures', 'Jerry Bruckheimer Fi...   \n",
       "\n",
       "                             production_countries     revenue  runtime  \\\n",
       "0  ['United States of America', 'United Kingdom']  2787965087    162.0   \n",
       "1                    ['United States of America']   961000000    169.0   \n",
       "\n",
       "     status  vote_average  vote_count  release_year  release_month  \\\n",
       "0  Released           7.2       11800          2009             12   \n",
       "1  Released           6.9        4500          2007              5   \n",
       "\n",
       "   release_quarter  \n",
       "0                4  \n",
       "1                2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"gs://movie-success-bucket/data/df_copy.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clustering_analysis(data_path):\n",
    "    \n",
    "#     # func_to_container_op requires packages to be imported inside of the function. \n",
    "#     import sys, subprocess;\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn']) \n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'matplotlib']) \n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'seaborn']) \n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorboard'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'wordcloud'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'fsspec'])\n",
    "\n",
    "#     import json\n",
    "#     import gcsfs\n",
    "#     import os\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import seaborn as sns\n",
    "#     import pickle\n",
    "#     import urllib\n",
    "#     import tensorflow \n",
    "#     import tensorflow.keras.backend as K\n",
    "#     from tensorflow.keras.layers import Layer, InputSpec\n",
    "#     from tensorflow.keras.layers import Dense, Input\n",
    "#     from tensorflow.keras.models import Model\n",
    "#     from tensorflow.keras.optimizers import SGD\n",
    "#     from tensorflow.keras import callbacks\n",
    "#     from tensorflow.keras.initializers import VarianceScaling\n",
    "#     from sklearn.cluster import KMeans  \n",
    "    \n",
    "    \n",
    "#     # Read data\n",
    "#     df=pd.read_csv(r'gs://movie-success-bucket/data/df_copy.csv')\n",
    "    \n",
    "#     del(df['Unnamed: 0'])\n",
    "    \n",
    "#     df_clust=df[['budget','popularity','revenue','runtime','vote_average','vote_count','release_quarter']]\n",
    "    \n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     mscaler=MinMaxScaler()\n",
    "    \n",
    "#     x=df_clust.values\n",
    "    \n",
    "#     x=mscaler.fit_transform(x)\n",
    "    \n",
    "#     # This function is to create an autoencoder for the DEC algorithm\n",
    "#     def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "#         \"\"\"\n",
    "#         Fully connected symmetric auto-encoder model.\n",
    "\n",
    "#         dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n",
    "#               dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n",
    "\n",
    "#         act: activation function\n",
    "\n",
    "#         return:\n",
    "#             (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n",
    "#         \"\"\"\n",
    "        \n",
    "#         n_stacks = len(dims) - 1\n",
    "        \n",
    "#         input_data = Input(shape=(dims[0],), name='input')\n",
    "#         x = input_data\n",
    "        \n",
    "#         # internal layers of encoder\n",
    "#         for i in range(n_stacks-1):\n",
    "#             x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "            \n",
    "#         # latent hidden layer\n",
    "#         encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n",
    "        \n",
    "#         x = encoded\n",
    "#         # internal layers of decoder\n",
    "#         for i in range(n_stacks-1, 0, -1):\n",
    "#             x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "            \n",
    "#         # decoder output\n",
    "#         x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "        \n",
    "#         decoded = x\n",
    "        \n",
    "#         autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n",
    "#         encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n",
    "        \n",
    "#         return autoencoder_model, encoder_model\n",
    "    \n",
    "#     n_clusters = 2\n",
    "#     n_epochs   = 100\n",
    "#     batch_size = 128\n",
    "    \n",
    "#     kmeans = KMeans(n_clusters=n_clusters, n_jobs=4)\n",
    "#     y_pred_kmeans = kmeans.fit_predict(x)\n",
    "    \n",
    "#     dims = [x.shape[-1], 500, 500, 2000, 10] \n",
    "#     init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "#                                distribution='uniform')\n",
    "#     pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "#     pretrain_epochs = n_epochs\n",
    "#     batch_size = batch_size\n",
    "# #     save_dir = 'C:/Users/m_joekid/Desktop/Vscodes'\n",
    "# #     save_dir = '{data_path}/Vscodes'\n",
    "\n",
    "#     autoencoder, encoder = autoencoder(dims, init=init)\n",
    "    \n",
    "#     autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "#     autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)\n",
    "# #     autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "    \n",
    "    \n",
    "# #     autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "# #     autoencoder.load_weights(save_dir + '/ae_weights.h5')\n",
    "    \n",
    "    \n",
    "#     # implementing DEC Soft Labeling\n",
    "#     class ClusteringLayer(Layer):\n",
    "        \n",
    "#         '''\n",
    "#         Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "#         sample belonging to each cluster. The probability is calculated with t-distribution.\n",
    "#         '''\n",
    "        \n",
    "#         def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "#             if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "#                 kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "#             super(ClusteringLayer, self).__init__(**kwargs)\n",
    "#             self.n_clusters = n_clusters\n",
    "#             self.alpha = alpha\n",
    "#             self.initial_weights = weights\n",
    "#             self.input_spec = InputSpec(ndim=2)\n",
    "            \n",
    "#         def build(self, input_shape):\n",
    "#             assert len(input_shape) == 2\n",
    "#             input_dim = input_shape[1]\n",
    "#             self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "#             self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n",
    "\n",
    "#             if self.initial_weights is not None:\n",
    "#                 self.set_weights(self.initial_weights)\n",
    "#                 del self.initial_weights\n",
    "#             self.built = True\n",
    "            \n",
    "#         def call(self, inputs, **kwargs):\n",
    "#             ''' \n",
    "#             t-distribution, as used in t-SNE algorithm.\n",
    "#             It measures the similarity between embedded point z_i and centroid µ_j.\n",
    "#                      q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "#                      q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "#                      (i.e., a soft assignment)\n",
    "\n",
    "#             inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "\n",
    "#             Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "#             '''\n",
    "            \n",
    "#             q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "#             q **= (self.alpha + 1.0) / 2.0\n",
    "#             q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n",
    "        \n",
    "#             return q\n",
    "\n",
    "#         def compute_output_shape(self, input_shape):\n",
    "#             assert input_shape and len(input_shape) == 2\n",
    "#             return input_shape[0], self.n_clusters\n",
    "        \n",
    "#         def get_config(self):\n",
    "#             config = {'n_clusters': self.n_clusters}\n",
    "#             base_config = super(ClusteringLayer, self).get_config()\n",
    "#             return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "#     # Once the soft labeling layer is defined, it can be used to form a DEC model\n",
    "#     clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "#     model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "    \n",
    "#     model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "    \n",
    "#     kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "#     y_pred = kmeans.fit_predict(encoder.predict(x))\n",
    "    \n",
    "#     y_pred_last = np.copy(y_pred)\n",
    "    \n",
    "#     model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "    \n",
    "#     # The DEC model is trained iteratively\n",
    "#     # computing an auxiliary target distribution\n",
    "#     def target_distribution(q):\n",
    "#         weight = q ** 2 / q.sum(0)\n",
    "#         return (weight.T / weight.sum(1)).T\n",
    "    \n",
    "#     loss = 0\n",
    "#     index = 0\n",
    "#     maxiter = 1000 # 8000\n",
    "#     update_interval = 100 # 140\n",
    "#     index_array = np.arange(x.shape[0])\n",
    "    \n",
    "#     tol = 0.001\n",
    "    \n",
    "#     for ite in range(int(maxiter)):\n",
    "#         if ite % update_interval == 0:\n",
    "#             q = model.predict(x, verbose=0) # verbose=0 to remove all the noise in the notebook\n",
    "#             p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "            \n",
    "#         idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "#         loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "#         index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "        \n",
    "# #     model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "    \n",
    "# #     model.load_weights(save_dir + '/DEC_model_final.h5')\n",
    "    \n",
    "#     # The new DEC model is trained iteratively\n",
    "#     # Eval.\n",
    "#     q = model.predict(x, verbose=0)\n",
    "#     p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "#     # evaluate the clustering performance\n",
    "#     y_pred = q.argmax(1)\n",
    "    \n",
    "#     df_copy=df.copy()\n",
    "    \n",
    "#     df_copy['cluster']=y_pred\n",
    "    \n",
    "#     # TSNE Dimensional reduction, similar to pca\n",
    "#     from sklearn.manifold import TSNE\n",
    "#     x_embedded = TSNE(n_components=2).fit_transform(x)\n",
    "#     x_embedded.shape\n",
    "    \n",
    "    \n",
    "#     plt.scatter(vis_x, vis_y, c=y_pred_kmeans, cmap=plt.cm.get_cmap(\"jet\", 256))\n",
    "#     plt.colorbar(ticks=range(3))\n",
    "#     plt.clim(-0.5, 9.5)\n",
    "#     plt.xlim(-60,10)\n",
    "#     plt.ylim(-60,40)\n",
    "#     plt.show()\n",
    "    \n",
    "#     vis_x = x_embedded[:, 0]\n",
    "#     vis_y = x_embedded[:, 1]\n",
    "#     plt.scatter(vis_x, vis_y, c=y_pred, cmap=plt.cm.get_cmap(\"jet\", 256))\n",
    "#     plt.colorbar(ticks=range(256))\n",
    "#     plt.clim(-0.5, 9.5)\n",
    "#     plt.xlim(-60,10)\n",
    "#     plt.ylim(-60,40)\n",
    "#     plt.show()\n",
    "    \n",
    "#     for num_clusters in range(2,10):\n",
    "#         clusterer = KMeans(n_clusters=num_clusters, n_jobs=4)\n",
    "#         preds = clusterer.fit_predict(x)\n",
    "#         # centers = clusterer.cluster_centers_\n",
    "#         score = silhouette_score(x, preds, metric='euclidean')\n",
    "#         print (\"For n_clusters = {}, Kmeans silhouette score is {})\".format(num_clusters, score))\n",
    "    \n",
    "#     # x_pca\n",
    "#     x_pca=pca.fit_transform(x)\n",
    "#     x_pca.shape\n",
    "    \n",
    "#     print(\"Clustering Analysis Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a pipeline component from the function\n",
    "\n",
    "### Convert the function to a pipeline operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preproces lightweight components.\n",
    "preprocess_data_analysis_op = comp.func_to_container_op(preprocess_data_analysis, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the analysis lightweight components.\n",
    "exploratory_data_analysis_op = comp.func_to_container_op(exploratory_data_analysis, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the clustering_analysis lightweight components.\n",
    "# clustering_analysis_op = comp.func_to_container_op(clustering_analysis, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create preproces lightweight components.\n",
    "preprocess_data_modeling_op = comp.func_to_container_op(preprocess_data_modeling, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the training lightweight components.\n",
    "model_training_op = comp.func_to_container_op(model_training, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the model evaluation lightweight components.\n",
    "model_validation_op = comp.func_to_container_op(model_validation, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the confusion matrix lightweight components.\n",
    "confusion_matrix_op = comp.func_to_container_op(confusion_matrix, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Kubeflow Pipeline\n",
    "\n",
    "\n",
    "- Our next step will be to create the various components that will make up the pipeline. Define the pipeline using the @dsl.pipeline decorator.\n",
    "\n",
    "\n",
    "- The pipeline function is defined and includes a number of paramters that will be fed into our various components throughout execution. Kubeflow Pipelines are created decalaratively. This means that the code is not run until the pipeline is compiled.\n",
    "\n",
    "\n",
    "- A Persistent Volume Claim can be quickly created using the VolumeOp method to save and persist data between the components.\n",
    "\n",
    "    - Note that while this is a great method to use locally, you could also use a cloud bucket for your persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain-specific language \n",
    "# Define the Pipeline\n",
    "@dsl.pipeline(\n",
    "    name='Movie Analysis Pipeline',\n",
    "    description='End-to-end Movie analysis machine learning Project pipeline.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def Movie_Analysis_container_pipeline(\n",
    "    data_path: str,  # DATA_PATH\n",
    "    model_file: str  # CLASSIFIER_PATH    \n",
    "):\n",
    "    \n",
    "    # Create a persistent volume\n",
    "    # Define volume to share data between components\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"creat_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    \n",
    "    # Define Pipeline Components and dependencies\n",
    "    # We do this with ContainerOp, an object that defines a pipeline component from a container.\n",
    "    \n",
    "    # Create movie success preprocessing component for analysis.\n",
    "    movie_success_preprocessing_data_analysis_container = preprocess_data_analysis_op(data_path) \\\n",
    "                                                            .add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    # Create movie success preprocessing component for modeling.\n",
    "    movie_success_preprocessing_data_modeling_container = preprocess_data_modeling_op(data_path) \\\n",
    "                                                            .add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    # Create movie success exploratory data analysis component\n",
    "    movie_success_exploratory_analysis_container = exploratory_data_analysis_op(data_path) \\\n",
    "                                        .add_pvolumes({data_path: movie_success_preprocessing_data_analysis_container.pvolume})\n",
    "    \n",
    "    # Create movie success exploratory data analysis component\n",
    "#     movie_success_clustering_analysis_container = clustering_analysis_op(data_path) \\\n",
    "#                                         .add_pvolumes({data_path: movie_success_preprocessing_data_analysis_container.pvolume})\n",
    "    \n",
    "    # Create movie success model training component\n",
    "    movie_success_model_training_container = model_training_op(data_path, model_file) \\\n",
    "                                        .add_pvolumes({data_path: movie_success_preprocessing_data_modeling_container.pvolume})\n",
    "    \n",
    "    # Create movie success model validation component\n",
    "    movie_success_model_validation_container = model_validation_op(data_path, model_file) \\\n",
    "                                        .add_pvolumes({data_path: movie_success_model_training_container.pvolume})\n",
    "     \n",
    "    # Create movie success confusion matrix component\n",
    "    movie_success_confusion_matrix_container = confusion_matrix_op(data_path, model_file) \\\n",
    "                                        .add_pvolumes({data_path: movie_success_model_validation_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    Movie_Analysis_container = dsl.ContainerOp(\n",
    "        name=\"Movie Analysis prediction\",  # the name displayed for the component execution during runtime.\n",
    "        image='library/bash:4.4.23',      # Image tag for the Docker container to be used.\n",
    "        pvolumes={data_path: movie_success_model_validation_container.pvolume}, # dictionary of paths and associated Persistent Volumes to be mounted to the container before execution.\n",
    "        arguments=['cat', f'{data_path}/classifier_result.txt'] # command to be run by the container at runtime.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and run the pipeline\n",
    "\n",
    "- Finally we feed our pipeline definition into the compiler and run it as an experiment. This will give us 2 links at the bottom that we can follow to the Kubeflow Pipelines UI where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline.\n",
    "\n",
    "- Kubeflow Pipelines lets you group pipeline runs by Experiments. You can create a new experiment, or call kfp.Client().list_experiments() to see existing ones. If you don't specify the experiment name, the Default experiment will be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some environment variables which are to be used as inputs at various points in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'  # mount your filesystems or devices\n",
    "CLASSIFIER_PATH = 'movie_analysis_main.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = Movie_Analysis_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1028: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/9cca4f23-2eda-48d5-b453-fd5c6ec7e723\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/f28c22dc-0bd8-47e8-886b-5e9fe7eceb07\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name=EXPERIMENT_NAME\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\":CLASSIFIER_PATH}\n",
    "\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,'{}.zip'.format(experiment_name))\n",
    "\n",
    "\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

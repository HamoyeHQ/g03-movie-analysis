{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Movie Success Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our lightweight pipelines components using Python\n",
    "\n",
    "\n",
    "#### Lightweight python components\n",
    "\n",
    "\n",
    "Lightweight python components do not require you to build a new container image for every code change. They're intended to use for fast iteration in notebook environment.\n",
    "\n",
    "#### Building a lightweight python component\n",
    "\n",
    "To build a component just define a stand-alone python function and then call kfp.components.func_to_container_op(func) to convert it to a component that can be used in a pipeline.\n",
    "\n",
    "There are several requirements for the function:\n",
    "\n",
    "- The function should be stand-alone. It should not use any code declared outside of the function definition. Any imports should be added inside the main function. Any helper functions should also be defined inside the main function.\n",
    "\n",
    "- The function can only import packages that are available in the base image. If you need to import a package that's not available you can try to find a container image that already includes the required packages. (As a workaround you can use the module subprocess to run pip install for the required package.)\n",
    "\n",
    "- If the function operates on numbers, the parameters need to have type hints. Supported types are [int, float, bool]. Everything else is passed as string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Python function-based components\n",
    "\n",
    "A Kubeflow Pipelines component is a self-contained set of code that performs one step in your ML workflow. A pipeline component is composed of:\n",
    "\n",
    "- The component code, which implements the logic needed to perform a step in your ML workflow.\n",
    "\n",
    "- A component specification, which defines the following:\n",
    "\n",
    "    - The component's metadata, its name and description.\n",
    "    - The component's interface, the component's inputs and outputs.\n",
    "    - The component's implementation, the Docker container image to run, how to pass inputs to your component code, and how to get the component's outputs.\n",
    "    \n",
    "\n",
    "Python function-based components make it easier to iterate quickly by letting you build your component code as a Python function and generating the component specification for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.3.1-py2.py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "Successfully installed pip-20.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.7.exe and pip3.exe are installed in 'C:\\Users\\Sillians\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sillians\\appdata\\roaming\\python\\python37\\site-packages (1.1.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "Collecting keras==1.2.2\n",
      "  Using cached Keras-1.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from keras==1.2.2) (5.1)\n",
      "Requirement already satisfied: six in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from keras==1.2.2) (1.12.0)\n",
      "Collecting matplotlib==3.3.1\n",
      "  Downloading matplotlib-3.3.1-1-cp37-cp37m-win_amd64.whl (8.9 MB)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\sillians\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib==3.3.1) (2020.11.8)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (1.0.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sillians\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib==3.3.1) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (2.3.1)\n",
      "Collecting pandas==0.23.4\n",
      "  Using cached pandas-0.23.4-cp37-cp37m-win_amd64.whl (7.9 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from pandas==0.23.4) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (1.18.5)\n",
      "Collecting scikit-learn==0.22\n",
      "  Using cached scikit_learn-0.22-cp37-cp37m-win_amd64.whl (6.2 MB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from scikit-learn==0.22) (0.14.1)\n",
      "Collecting scipy==1.2.1\n",
      "  Using cached scipy-1.2.1-cp37-cp37m-win_amd64.whl (30.0 MB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (1.18.5)\n",
      "Collecting seaborn==0.10.1\n",
      "  Downloading seaborn-0.10.1-py3-none-any.whl (215 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from matplotlib==3.3.1) (1.18.5)\n",
      "Collecting tensorflow==2.1.0\n",
      "  Downloading tensorflow-2.1.0-cp37-cp37m-win_amd64.whl (355.8 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from tensorflow==2.1.0) (0.33.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from tensorflow==2.1.0) (0.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "INFO: pip is looking at multiple versions of seaborn to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested scipy==1.2.1\n",
      "    scikit-learn 0.22 depends on scipy>=0.17.0\n",
      "    seaborn 0.10.1 depends on scipy>=1.0.1\n",
      "    tensorflow 2.1.0 depends on scipy==1.4.1; python_version >= \"3\"\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/87/67/e18ed11f774f42f9440eba7787af3104fb7d18c9bc4fbc19fae58ebe3df1/matplotlib-3.3.1-1-cp37-cp37m-win_amd64.whl\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/87/67/e18ed11f774f42f9440eba7787af3104fb7d18c9bc4fbc19fae58ebe3df1/matplotlib-3.3.1-1-cp37-cp37m-win_amd64.whl\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/87/67/e18ed11f774f42f9440eba7787af3104fb7d18c9bc4fbc19fae58ebe3df1/matplotlib-3.3.1-1-cp37-cp37m-win_amd64.whl\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/34/d5/ce8c17971067c0184c9045112b755be5461d5ce5253ef65a367e1298d7c5/tensorflow-2.1.0-cp37-cp37m-win_amd64.whl\n",
      "ERROR: Cannot install scikit-learn==0.22, scipy==1.2.1, seaborn==0.10.1 and tensorflow==2.1.0 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn==0.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (0.0)\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested IPython==7.12.0\n",
      "    The user requested IPython==7.11.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "ERROR: Cannot install IPython==7.11.1 and IPython==7.12.0 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==2.3.2\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-win_amd64.whl (9.3 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (41.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (2.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (4.42.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (2.22.0)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached blis-0.4.1-cp37-cp37m-win_amd64.whl (5.0 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (1.18.5)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (3.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sillians\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (1.25.10)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (3.0.4)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-win_amd64.whl (176 kB)\n",
      "Collecting thinc==7.4.1\n",
      "  Using cached thinc-7.4.1-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (1.18.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (2.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sillians\\anaconda3\\lib\\site-packages (from spacy==2.3.2) (4.42.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: wasabi, srsly, catalogue, blis, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 spacy-2.3.2 srsly-1.0.5 thinc-7.4.1 wasabi-0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/9b/ce/ddac37d457ae17152bc7e15164a11bf8236fc4e8a05cabb94d922f58ea23/spacy-2.3.2-cp37-cp37m-win_amd64.whl\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/9b/ce/ddac37d457ae17152bc7e15164a11bf8236fc4e8a05cabb94d922f58ea23/spacy-2.3.2-cp37-cp37m-win_amd64.whl\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/9b/ce/ddac37d457ae17152bc7e15164a11bf8236fc4e8a05cabb94d922f58ea23/spacy-2.3.2-cp37-cp37m-win_amd64.whl\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/9b/ce/ddac37d457ae17152bc7e15164a11bf8236fc4e8a05cabb94d922f58ea23/spacy-2.3.2-cp37-cp37m-win_amd64.whl\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython \n",
    "!python -m pip install pandas \n",
    "!pip install pandas==0.23.4 matplotlib==3.3.1 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.1.0 keras==1.2.2 seaborn==0.10.1 --user\n",
    "!pip install IPython==7.12.0 numpy==1.16.1 imblearn==0.0 jsonlib==1.6.1 tensorboard==2.2.0 wordcloud==1.8.0 IPython==7.11.1 --user\n",
    "!pip install spacy==2.3.2 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install or update the pipelines SDK\n",
    "\n",
    "\n",
    "### Run the following command to install the Kubeflow Pipelines SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "!pip3 install --user --upgrade kfp\n",
    "!pip3 install kfp --upgrade\n",
    "!pip3 install kfp --upgrade --user\n",
    "!pip3 install -U kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel before you proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after the pip install\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Components\n",
    "\n",
    "### Import the kfp and kfp.components packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp                  # the Pipelines SDK. \n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from kfp.dsl.types import Integer, GCSPath, String\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/g03-movie-success/data/out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a release experiment in the Kubeflow pipeline\n",
    "\n",
    "#### Kubeflow Pipeline requires having an Experiment before making a run. An experiment is a group of comparable runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Movie Success Pipeline'        # Name of the experiment in the UI\n",
    "BASE_IMAGE = \"tensorflow/tensorflow:latest-gpu-py3\"    # Base image used for components in the pipeline\n",
    "\n",
    "PROJECT_NAME = \"Kubeflow-mlops-pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the kfp.Client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"movie-success-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"gs://{}/data/merged_movies_dataset.csv\".format(bucket))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Python function-based components\n",
    "\n",
    "### Define your component's code as a standalone python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pandas import Series, DataFrame,read_csv\n",
    "    import pickle\n",
    "    \n",
    "    # read data\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # remove not required columns\n",
    "    data = data.drop('original_title', axis = 1, inplace = True)\n",
    "    \n",
    "    # print the first 5 rows\n",
    "    print(data.head())\n",
    "    \n",
    "    # Handling the Json Columns\n",
    "    # Applying the literal_eval function of ast on all the json columns\n",
    "    json_cols = ['cast','crew','genres','keywords','production_companies','production_countries','spoken_languages']\n",
    "    for col in json_cols:\n",
    "        data[col] = data[col].apply(literal_eval)\n",
    "        \n",
    "    # Helper Functions for the same\n",
    "    # function to get the names of the movies genre\n",
    "    def get_genre(x):\n",
    "        if(isinstance(x, list)):\n",
    "            genre = [i['name'] for i in x]\n",
    "    \n",
    "    return genre\n",
    "\n",
    "    # function to get the jobs of the crew members \n",
    "    def get_jobs(x):\n",
    "        if(isinstance(x, list)):\n",
    "            jobs = [i['job'] for i in x]\n",
    "    return jobs\n",
    "\n",
    "    # function to get the target/label (Animation == 1 / Not_Animation == 0)\n",
    "    def get_labels(x):\n",
    "        if(len(x)==0):\n",
    "            return np.nan\n",
    "    elif('Animation' in x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    # Get percentage of voice artists among total cast\n",
    "    def get_characternames(x):\n",
    "        if(isinstance(x, list)):\n",
    "            chr_name = [i['character'] for i in x]\n",
    "            countc = 0\n",
    "            for j in chr_name:\n",
    "                if('(voice)' in j):\n",
    "                    countc += 1\n",
    "            if(len(chr_name)!=0):\n",
    "                return (countc/len(chr_name))\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # function to get crew memebers whose jobs are Costume Design\n",
    "    def get_costume_labels(x):\n",
    "        if 'Costume Design' in x:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    # function to get the genre department with the Lighting role\n",
    "    def get_genre_cd(x):\n",
    "        if(isinstance(x, list)):\n",
    "            dept = [i['department'] for i in x]\n",
    "        if 'Lighting' in dept:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    # Applying the above functions \n",
    "    data['genres'] = data['genres'].apply(get_genre)\n",
    "    data['crew_jobs'] = data['crew'].apply(get_jobs)\n",
    "    data['percent_of_voice_artists'] = data['cast'].apply(get_characternames)\n",
    "    data['labels'] = data['genres'].apply(get_labels)\n",
    "    \n",
    "    # Rounding off the percentage to 3 decimal places\n",
    "    for x in range(0,len(data['percent_of_voice_artists'])):\n",
    "        data['percent_of_voice_artists'][x] = np.round(data['percent_of_voice_artists'][x],3)\n",
    "        \n",
    "    # number of Labels missing / Null values  \n",
    "    data.labels.isna().sum()\n",
    "    \n",
    "    \n",
    "    # dealing with Labels missing values\n",
    "    idxsc = data[((data.labels != 1) & (data.labels != 0))].index\n",
    "    data.drop(idxsc, inplace = True)\n",
    "    data.reset_index(drop= True, inplace= True)\n",
    "    \n",
    "    # checking for dataset Features with missing values\n",
    "    data.isna().sum()\n",
    "    \n",
    "    # check the number of animated and non_animated movies\n",
    "    AnimatedMoviesCount = np.sum(data['labels'] == 1)\n",
    "    NotAnimatedMoviesCount = np.sum(data['labels'] == 0)\n",
    "\n",
    "#     print(\"Number of Animated Movies are: \", AnimatedMoviesCount)\n",
    "#     print(\"Number of Not Animated Movies are: \", NotAnimatedMoviesCount)\n",
    "\n",
    "    # Apply the get_costume_labels function\n",
    "    data['costume'] = data['crew_jobs'].apply(get_costume_labels)\n",
    "    \n",
    "    data.costume.value_counts()\n",
    "    \n",
    "    # Apply get_genre_cd function\n",
    "    data['lighting_dept'] = data['crew'].apply(get_genre_cd)\n",
    "\n",
    "    data.lighting_dept.value_counts()\n",
    "    \n",
    "    # Taking into account only those movies having atleast 7 crew members\n",
    "    # So as to handle the quality of training data Tested for multiple values, but 7 yielded best result\n",
    "    idx=[]\n",
    "    for x in range(0,data.shape[0]):\n",
    "        if len(data.crew_jobs[x])>7:\n",
    "            idx.append(x)\n",
    "    print(\"Number of Movies with more than 7 crew members: \",str(len(idx)))\n",
    "\n",
    "    df = data.iloc[idx,:]\n",
    "    \n",
    "    \n",
    "    # Get the number of animated and non_animated movies\n",
    "    AnimatedMoviesCount2 = np.sum(df['labels'] == 1)\n",
    "    NotAnimatedMoviesCount2 = np.sum(df['labels'] == 0)\n",
    "    \n",
    "    print(\"Number of Animated Movies are: \", AnimatedMoviesCount2)\n",
    "    print(\"Number of Not Animated Movies are: \", NotAnimatedMoviesCount2)\n",
    "    \n",
    "    \n",
    "    # Converting 'crew_jobs' from list to string (in lower form) via join function\n",
    "    def join_strings(x):\n",
    "        return \", \".join(x)\n",
    "\n",
    "    def str_lower(x):\n",
    "        return x.lower()\n",
    "\n",
    "    df['crew_jobs'] = df['crew_jobs'].apply(join_strings)\n",
    "    df['crew_jobs'] = df['crew_jobs'].apply(str_lower)\n",
    "    \n",
    "    # get the number of labels\n",
    "    df['labels'].value_counts() \n",
    "    \n",
    "    #Save preprocessed data\n",
    "    df.to_csv(\"data/preprocessed\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"gs://{}/data/merged_movies_dataset.csv\".format(bucket)\n",
    "\n",
    "preprocess(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed data to google cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp data/preprocessed gs://${bucket}/data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where preprocessed data is stored\n",
    "in_dir = \"gs://{}/data/preprocessed\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def model_train(out_data_path, model_dir) -> NamedTuple(\n",
    "    'TrainingOutput',\n",
    "    [\n",
    "        ('mlpipeline_ui_metadata', 'UI_metadata')\n",
    "#         ('mlpipeline_metrics', 'Metrics')\n",
    "    ]):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy==1.16.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'imblearn==0.0']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib==1.6.1']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorboard==2.1.0'])  \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'IPython==7.12.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy==2.3.2'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import imblearn\n",
    "    import spacy\n",
    "    from spacy.lang.en import STOP_WORDS\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    \n",
    "    # get data\n",
    "    df = pd.read_csv(\"gs://movie-success-bucket/data/preprocessed\")\n",
    "    \n",
    "    # Get the features and labels\n",
    "    X = df['crew_jobs']\n",
    "    y = df['labels']\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=53)\n",
    "    \n",
    "    # function to output our scores \n",
    "    def score_output(y_test, y_pred):\n",
    "        print(metrics.confusion_matrix(y_test, y_pred))\n",
    "        print(metrics.classification_report(y_test, y_pred))\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('The Accuracy on The Test Set is: %s' % accuracy)\n",
    "        \n",
    "    # model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # instantiate stopwords to use\n",
    "    stop_words_str = \" \".join(STOP_WORDS)\n",
    "    stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "    additional_words = ['editor', 'director', 'producer', 'writer', 'assistant', 'sound']\n",
    "\n",
    "    for word in additional_words:\n",
    "        stop_words_lemma = stop_words_lemma.union({word})\n",
    "        \n",
    "    # define the lemmatizer function\n",
    "    def lemmatizer(text):\n",
    "        return [word.lemma_ for word in nlp(text)]\n",
    "    \n",
    "    # Without Stop Words\n",
    "    bow = TfidfVectorizer(ngram_range = (1,1))\n",
    "\n",
    "    model = Pipeline([('bag_of_words', bow),('classifier', SVC())])\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    print(\"Without Stop Words\")\n",
    "    print('Training accuracy: {}'.format(model.score(X_train,y_train)))\n",
    "    y_pred = model.predict(X_test)\n",
    "    score_output(y_test, y_pred)\n",
    "    \n",
    "    # output the splitted data file to path \n",
    "    np.savez_compressed(f'{out_data_path}/train-test-data.npz', \n",
    "                       X_train=x_train,\n",
    "                       X_test=x_test,\n",
    "                       y_train=y_train,\n",
    "                       y_test=y_test)\n",
    "    \n",
    "    #Save the model as a pickle file.\n",
    "    with open(f'{out_data_path}/model_file', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "    # Save the classifier model to the designated \n",
    "#     with open(f'{data_path}/{classifier_file}', 'wb') as file:\n",
    "#         pickle.dump(classifier, file)\n",
    "    \n",
    "    \n",
    "#     from collections import namedtuple\n",
    "#     training_output = namedtuple(\n",
    "#         'TrainingOutput',\n",
    "#         ['model', 'mlpipeline_ui_metadata']) \n",
    "#     return training_output(model, json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = train(out_dir, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export saved model to google cloud storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {out_dir}/model gs://${bucket}/{out_dir}/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def model_validation(data_path, classifier_file) -> NamedTuple(\n",
    "    'ModelvalidationOutputs',\n",
    "    [\n",
    "      ('recall', float),\n",
    "      ('accuracy', float),\n",
    "      ('precision', float),\n",
    "      ('f1score', float),\n",
    "#       ('mlpipeline_ui_metadata', 'UI_metadata'),\n",
    "      ('mlpipeline_metrics', 'Metrics')\n",
    "    ]):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy==1.16.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib==1.6.1']) \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import pickle\n",
    "    from sklearn.metrics import classification_report, recall_score, accuracy_score,precision_score, f1_score, confusion_matrix\n",
    "    \n",
    "    \n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/model_file','rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    # load the transformed data\n",
    "    train_test_data = np.load(f'{out_data_path}/train-test-data.npz')\n",
    "    X_train = train_test_data['X_train']\n",
    "    X_test  = train_test_data['X_test']\n",
    "    y_train = train_test_data['y_train']\n",
    "    y_test  = train_test_data['y_test']\n",
    "        \n",
    "    # function to output our scores \n",
    "    def score_output(y_test, y_pred):\n",
    "        print(metrics.confusion_matrix(y_test, y_pred))\n",
    "        print(metrics.classification_report(y_test, y_pred))\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('The Accuracy on The Test Set is: %s' % accuracy)\n",
    "        \n",
    "    # write out metrics\n",
    "    accuracy = model.score(X_train,y_train)\n",
    "    \n",
    "    print(\"Without Stop Words\")\n",
    "    print('Training accuracy: {}'.format(model.score(X_train,y_train)))\n",
    "    y_pred = model.predict(X_test)\n",
    "    score_output(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exploratory_data_analysis(data_path):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function. \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'matplotlib==3.3.1']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'seaborn==0.10.1']) \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'jsonlib==1.6.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'wordcloud==1.8.0'])\n",
    "    import json\n",
    "    import ast\n",
    "    from wordcloud import WordCloud\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import json\n",
    "    import pickle\n",
    "    import urllib\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # analysis to get the Average Budget of Animated Movie\n",
    "    c = np.where(data.labels==1)[0]\n",
    "    sum_budget = 0\n",
    "    for x in c:\n",
    "        sum_budget += data.budget[x]\n",
    "    avg_budget = sum_budget/len(c)\n",
    "    print(\"Average Budget of Animated Movie: \",str(avg_budget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
